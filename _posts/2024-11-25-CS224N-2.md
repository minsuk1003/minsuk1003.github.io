---
layout: post
title: '[NLP] CS224N - [2] Word Vectors: GloVe'
date: 2024-11-25 22:00:00
categories: [NLP, CS224N]
tags: [Word Vectors, Co-occurrence, GloVe]
math: true
toc: true
render_with_liquid: true
image:
  path: assets/img/for_post/241125-t.jpg
img_path: /assets/img/for_post/
description: CS224N 2강을 정리한 글로, 카운트 기반 학습 방법을 토대로 Word2Vec을 보완한 GloVe 모델에 대해 설명한다.
---

## CS224N Series

- [ ] [[1] Word Vectors: Word2Vec](https://minsuk1003.github.io/posts/CS224N-1/)
- [x] <span style="color: #07a8f7">[2] **Word Vectors: GloVe**</span>
- [ ] [3] Word Senses
- [ ] [4] Backprop and Neural Networks
- [ ] [5] Syntactic Structure and Dependency Parsing
- [ ] [6] Recurrent Neural Networks (RNNs)
- [ ] [7] LSTM RNNs
- [ ] [8] Seq2Seq, Attention
- [ ] [9] Self-Attention and Transformers

---

> 본 포스팅은 CS224N 2강에서 다룬 단어 벡터 학습 방법을 토대로 Word2Vec을 개선한 GloVe 모델, 그리고 평가 방법을 설명한다.
{: .prompt-info}

---
## Word Prediction Methods

Word Vector를 학습하는 방법은 크게 2가지로 나눌 수 있다.

1. **Count-based**: 단어 간 등장 횟수를 기반으로 관계 모델링
   - 빠른 학습 속도
   - 효율적인 통계 정보 활용
   - 단어간 관계 고려 불가
   - 큰 빈도수에 과도한 중요도 부여
   - ex) LSA, HAL, COALS 등
   
2. **Direct**: 단어를 직접 예측하며 관계 학습
   - Corpus size에 따라 성능 영향
   - 비효율적인 **통계 정보** 활용
   - 대부분의 영역에서 높은 성능
   - 단어 간 유사성 이상의 복잡한 패턴 포착 가능
   - ex) Word2Vec, NNLM, Neural Networks 등

Direct Prediction 방법이 기본적으로 더 고도화된 학습 방법이지만, **통계 정보를 효율적으로 활용하지 못하는 단점**은 존재한다.

특히, Word2Vec은 윈도우 크기 내의 주변 단어만을 고려하므로 전체적인 통계 정보를 반영하지 못한다.

> GloVe 모델은 Count-based와 Direct Prediction 방법을 모두 사용하여 Word2Vec의 단점을 보완한다.
{: .prompt-tip}

---
### Co-occurrence Matrix

**동시 등장 행렬 <sup>Co-occurrence Matrix</sup>** 는 단어 간의 관계를 표현하기 위해 가장 기본적으로 활용되는 방법이다.

행과 열을 전체 단어 집합으로 구성하고, 윈도우 크기 내에서 단어가 등장한 횟수를 기재한다.

**Example corpus**: "I like deep learning. I like NLP."  

위의 예시 문장에서 윈도우 크기 <sup>window size</sup> 를 1로 가정했을 때, 다음과 같은 Co-occurrence Matrix를 구할 수 있다.

|          | I   | like | deep | learning | NLP |
|----------|-----|------|------|----------|-----|
| **I**    | 0   | 2    | 0    | 0        | 0   |
| **like** | 2   | 0    | 1    | 0        | 1   |
| **deep** | 0   | 1    | 0    | 1        | 0   |
| **learning** | 0 | 0    | 1    | 0        | 0   |
| **NLP**  | 0   | 1    | 0    | 0        | 0   |

서로 가까이 배치되어 유사한 의미를 가진 단어들은 비슷한 벡터를 가지게 된다.

> 그러나, 단어의 수가 증가할 수록 행렬의 차원이 커지며, Sparsity 이슈로 모델이 불안정한 문제가 발생한다.
{: .prompt-warning}

이러한 높은 복잡도를 해결하기 위해 **Singular Value Decomposition (SVD)**를 통해 차원 축소를 수행할 수 있다.

![image](assets/img/for_post/241125-1.png)
_Singular Value Decomposition_

---
### Co-occurrence Probabilities

**동시 등장 확률 <sup>Co-occurrence Probabilities</sup>** $P(k \mid i)$은 특정 단어 $i$가 등장했을 때 다른 단어 $k$가 등장한 확률이다.

앞선 예시를 기반으로, like 단어가 등장했을 때의 NLP 단어의 등장 확률은 다음과 같이 계산될 수 있다.

$$
P(\text{NLP} \mid \text{like}) = \frac{X_{\text{like}, \text{NLP}}}{\sum_{n} X_{\text{like}, n}} = \frac{X_{\text{like}}} {X_{\text{like}, \text{I}} + X_{\text{like}, \text{deep}} + X_{\text{like}, \text{NLP}}} = \frac{1}{2+1+1} = 0.25
$$

동시 등장 확률의 비율은 단어 간 의미적 관계를 더욱 잘 나타낼 수 있다.

예를 들어, \(P(\text{NLP} \mid \text{I}) = 0\)인 것을 바탕으로, NLP는 **I보다 like와 더 가까운 의미적 관계**를 가지는 것을 알 수 있다.

이러한 정보를 바탕으로 GloVe는 Word Vector를 학습한다.

&nbsp;

---
## Glove

> **GloVe (Global Vectors for Word Representation)**는 단어 간 동시 등장 확률의 비율을 학습하여 단어 벡터를 생성하는 모델이다. [^ref1]
{: .prompt-tip}

### Overview

GloVe의 핵심 아이디어는 단어 $i$와 단어 $j$ 간의 관계를 나타내는 동시 등장 행렬 $X_{ij}$를 기반으로 단어 벡터를 학습한다.

$$
w_i^\top w_j + b_i + b_j = \log(X_{ij})
$$
- $w_i, w_j$: 각 단어의 벡터  
- $b_i, b_j$: 각 단어의 바이어스
- $X_{ij}$: 단어 $i$와 $j$의 동시 등장 횟수

> 왜 로그를 사용하는가?
> - $X_ij$의 값은 범위가 매우 넓을 수 있어 로그를 취해 0이 되는 경우를 방지하며, 학습을 안정화할 수 있다.
{: .prompt-tip}


### Objective Function

GloVe는 다음 손실 함수를 최소화하여 단어 벡터를 학습한다.

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) \left( w_i^\top w_j + b_i + b_j - \log(X_{ij}) \right)^2
$$

이 때, 동시 등장 행렬 $X$는 희소 행렬 <sup>Sparse Matrix</sup> 일 가능성이 있어 **가중치 함수 <sup>Weighting Function</sup>** $f(X_{ij})$를 손실 함수에 도입한다.

가중치 행렬은 보다 드물게 등장하지만 중요한 의미 관계를 나타내는 단어 쌍에 집중한다.

동시 출현 빈도가 높은 단어 쌍에 낮은 가중치를 부여하고, 동시 출현 빈도가 낮은 단어 쌍에 높은 가중치를 부여한다.

$$
f(X_{ij}) = 
\begin{cases} 
\left(\frac{X_{ij}}{X_{\text{max}}}\right)^{0.75} & \text{if } X_{ij} < X_{\text{max}} \\ 
1 & \text{otherwise}
\end{cases}
$$

![image](assets/img/for_post/241125-2.png)
_Weighting Function_

---
### Code

```python
!pip install glove_python_binary

from glove import Corpus, Glove

corpus = Corpus() 

# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성
corpus.fit(result, window=5)
glove = Glove(no_components=100, learning_rate=0.05)

# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
```

&nbsp;

---
## Evaluation of Word Vectors

단어 벡터를 평가하기 위한 2가지 방식이 존재한다.

1. **Extrinsic Evaluation**
   - 도출한 단어 벡터를 현실 문제에 적용하는 방법
   - 각 epoch마다 평가를 해야 하므로 시간이 오래 걸림
   - 모델 구조의 문제인지, 임베딩의 문제인지 평가가 어려움
   
   CoNLL 데이터셋에 대해 NER 작업을 수행한 결과, Glove가 다른 모델을 능가했다.

2. **Intrinsic Evaluation**
   - 중간 단계의 구체적인 subtask를 통해 성능을 평가하는 방법
   - 빠른 평가가 가능하지만, 현실 문제 적용에 대한 판단이 어려움
   
   Word Analogies, Correlation 등에 대해 평가한 결과, Glove가 다른 모델을 능가했다.
   - Word Analogies 
     - A : B = C : ? 를 유추하는 문제
   - Correlation
     - 단어 벡터 간의 상관관계 평가

---
## 결론

GloVe는 Co-occurrence 통계 정보를 토대로 단어 간의 글로벌 컨텍스트를 학습하여 의미적 유사성을 잘 반영하는 단어 벡터를 생성함으로써 Word2Vec의 한정된 로컬 정보 중심 학습을 보완할 수 있다.

이를 통해, 상대적으로 빠른 학습 속도를 가지며, 대량의 텍스트에 대해서 확장성을 가진다.

---
## Reference

[CS224N: Lecture 2](https://youtu.be/gqaHkPEZAew?si=eC84KSwbTP6LRSok)

[딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22885)

[^ref1]: [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf)
