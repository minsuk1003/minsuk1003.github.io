---
layout: post
title: '[NLP] CS224N - [1] Word Vectors'
date: 2024-11-05 22:40:00
categories: [NLP, CS224N]
tags: [WordNet, One-hot Vector, Distributional Semantics, Word Vectors, Word2Vec]
math: true
toc: true
render_with_liquid: true
img_path: /assets/img/for_post/
---
## CS224N Series

- [x] <span style="color: #07a8f7">[1] **Word Vectors**</span>

---

> 본 포스팅은 CS224N 1강을 정리한 글로, 단어의 의미를 표현하는 Word Vector에 대해 설명한다.
{: .prompt-info}

---
## WordNet

단어의 의미를 표현하는 가장 초기의 방법 중 하나인 **WordNet**은 단어 간의 관계를 명시적으로 제공한다.

> WordNet은 단어 간 **유사도, 동의어 관계 <sup>Synonym Sets</sup>, 상위-하위 관계 <sup>Hypernyms</sups>**를 구조적으로 정의한 사전이다.
{: .prompt-tip}

### WordNet Structure

- **Synonym Sets (Synsets)** | 동의어 집합
  - $good \approx beneficial \approx adventageous$
- **Hypernyms** | "is a" 관계
  - $panda \subset mammal \subset animal $

### Code

```python
from nltk.corpus import wordnet as wn

# "good" 단어의 동의어 집합 확인
synsets = wn.synsets("good")
for s in synsets:
    print(s)
```

> 하지만, WordNet은 몇 가지 문제점을 안고 있다.
{: .prompt-warning}

1. **세부 의미 부족** | 단어의 문맥을 제대로 반영하지 못함
   - `good`과 `beneficial`은 미묘하게 다른 의미를 갖지만, 항상 동일시함
2. **새로운 단어 부족** | 새로 등장하는 단어를 빠르게 반영하지 못함
   - `wicked`, `badass`, `bombest` 등의 새로운 단어를 상시 추가해야 하는데 번거로움
3. **주관적** | 사전에 추가할 때 주관이 반영됨
4. **수동 구축** | 인간의 노력이 필요하므로 확장이 어려움

&nbsp;

---
## One-hot Vector

WordNet의 문제를 극복하기 위해 컴퓨터가 단어를 더 효율적으로 다루기 위해 **단어를 벡터로 표현하는 시도**들이 등장하였다.

단어를 벡터로 표현하는 가장 기본적인 방법인 **One-hot Vector**는 각 단어를 고유한 인덱스로 표현한다.

벡터 공간에서 하나의 차원만 1로 활성화되며, 나머지는 모두 0으로 채워진다.

![image](assets/img/for_post/241105-1.png)
_One-hot Vector_

> 단어를 단순히 고유 인덱스로 표현하므로, 단어의 수는 곧 벡터의 차원 수가 되어 차원이 기하급수적으로 증가한다.
{: .prompt-warning}

또한, `motel`과 `hotel`처럼 의미적으로 비슷한 단어도 **벡터 공간에서는 orthogonal하여 유사도를 측정할 수 없다.**

이 문제를 해결하기 위해, 단어의 문맥을 활용하여 의미를 나타내는 새로운 접근법이 필요하게 되었다.

&nbsp;

---
## Distributional Semantics

> **Distributional Semantics**는 단어의 의미를 문맥에서 찾는 접근법으로, 함께 사용되는 단어를 통해 특정 단어의 의미를 알 수 있다는 아이디어에 기반한다.
{: .prompt-tip}

> _"You shall know a word by the company it keeps."_ - J.R. Firth 1957

![image](assets/img/for_post/241105-2.png)
_특정 단어의 의미를 결정하는 주변 단어_

특정 단어 $w$의 의미는 고정된 길이 내의 단어들에 의해 결정된다.

&nbsp;

---
## Word Vectors

하지만, Distribution Semantics는 여전히 단어 문맥 정보를 수작업으로 처리하거나, 단순 통계에 의존한다.

더욱 정교한 표현을 위해 단어를 실수 벡터로 표현하는 방법이 등장하게 된다.

> Word Vectors는 단어를 고차원 실수 벡터로 표현하여 단어 간 의미적 유사도를 수치적으로 표현할 수 있다.
{: .prompt-tip}

의미적으로 유사한 단어들은 벡터 공간에서 가깝게 위치하도록 학습된다.

![image](assets/img/for_post/241105-3.png)
_유사 단어에 대한 가까운 벡터 표현_

Word Vector는 다음의 특징을 가진다.

- **Dense Representation** | 대부분의 값이 0이 아닌 밀집 벡터이다.
- **Similarity Measurement** | 벡터 간 내적 또는 코사인 방법을 통해 유사도를 계산할 수 있다.

&nbsp;

---
## Word2Vec

> Word2Vec는 단어 벡터를 학습하는 주요 방법으로, 단어의 의미적 유사성을 벡터 공간에 잘 반영하도록 학습된다. [^ref1]
{: .prompt-tip}

### Overview

Word2Vec은 두 가지 학습 방식을 제공한다.

1. **Skip-gram** | 중심 단어로 주변 단어 예측
2. **CBOW (Continuous Bag of Words)** | 주변 단어들로 중심 단어 예측

본 포스팅은 **Skip-gram**을 기준으로 설명하며, 목표는 중심 단어 $c$를 기준으로 주변 단어 $o$를 예측하는 **조건부 확률** \(P(o \mid c)\)를 학습하는 것이다.

![image](assets/img/for_post/241105-4.png)
_중심 단어를 활용한 주변 단어 확률 계산_

### Objective Function

주어진 데이터 $D$가 $T$개의 단어로 이루어져 있고, 각 중심 단어 $w_t$에 대해 윈도우 크기 $m$만큼의 주변 단어를 고려했을 때,  
전체 우도 <sup>Likelihood</sup>는 모든 중심 단어와 그 주변 단어들의 조건부 확률을 곱한 값이다.

$$
L(\theta) = \prod_{t=1}^T \prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j} \mid w_t; \theta)
$$

- \( w_t \): 시점 \(t\)의 중심 단어
- \( w_{t+j} \): 중심 단어 주변에 있는 \(j\)-번째 단어
- \( \theta \): 학습할 모든 파라미터 (단어 벡터 \(u\), \(v\))

Word2Vec은 미분이 용이하기 위해 로그 우도 <sup>Log Likelihood</sup>를 최대화하는 문제로 변환한다.

$$
\log L(\theta) = \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} \mid w_t; \theta)
$$

로그 우도의 부호를 반전시켜 **목적 함수 <sup>Objective Function</sup>**를 최소화하는 문제로 변환한다.

$$
J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} \mid w_t; \theta)
$$

목적 함수를 최소화하기 위해, 조건부 확률 $P(w_{t+j} \mid w_t; \theta$을 계산해야 한다.

소프트맥스 함수를 통해 다음과 같이 계산할 수 있다.

$$
P(o \mid c) = \frac{\exp(u_o^\top v_c)}{\sum_{w \in V} \exp(u_w^\top v_c)}.
$$

1. **유사도 계산**: 중심 단어 \(c\)의 벡터 \(v_c\)와 주변 단어 \(o\)의 벡터 \(u_o\) 간의 내적 (\(u_o^\top v_c\))  
2. **지수 함수 적용**: 양수로 만들기 위해 내적 값을 지수화  
3. **정규화**: 전체 단어 집합 \(V\)에 대해 확률 분포를 만들기 위해 정규화

![image](assets/img/for_post/241105-5.png)
_소프트맥스 함수 계산 과정_

### Optimization

Word2Vec은 경사 하강법 <sup>Gradient Descent</sup>을 통해 목적 함수 $J(\theta)$를 최소화한다.

경사 하강법을 통해 주변 단어와 중심 단어 벡터에 대한 기울기를 갱신한다.

1. 주변 단어 벡터 $u_o$에 대한 기울기
$$
\frac{\partial J}{\partial u_o} = v_c - P(o \mid c)v_c
$$

2. 중심 단어 벡터 $v_c$에 대한 기울기 
$$
\frac{\partial J}{\partial v_c} = u_o - \sum_{w \in V} P(w \mid c) u_w
$$

3. 각 벡터 업데이트
$$
v_c \leftarrow v_c - \eta \frac{\partial J}{\partial v_c}, \quad u_o \leftarrow u_o - \eta \frac{\partial J}{\partial u_o}
$$
- \( \eta \): 학습률 (Learning Rate) 

> 그러나, 경사 하강법을 적용하면, 단어의 개수가 많을수록 소프트맥스 함수에 대한 계산량이 기하급수적으로 증가하며, 학습 속도가 매우 느려진다.
{: .prompt-warning}

Word2Vec에서는 이러한 문제를 해결하기 위해 확률적 경사 하강법 <sup>Stochastic Gradient Descent</sup>를 활용한다.

전체 데이터셋 대신 작은 배치에 대해 기울기를 계산하고, 파라미터를 업데이트한다.

$$
v_c \leftarrow v_c - \eta \frac{\partial J_{sample}}{\partial v_c}, \quad u_o \leftarrow u_o - \eta \frac{\partial J_{sample}}{\partial u_o}
$$  
- \(J_{sample}\): 단일 샘플(또는 미니배치)에 대한 손실 함수  
- \(\eta\): 학습률


---
## 결론

단어 의미를 표현하는 다양한 방법을 살펴보았다. 단순한 WordNet, One-hot Vectors를 시작으로 문맥을 반영하는 Distributional Semantic, 그리고 이를 신경망 기반으로 학습하는 Word2Vec까지 발전하는 과정을 이해할 수 있었다.

---
## Reference

1. [^ref1]: [Word2Vec: Efficient Estimation of Word Representations in Vector Space](https://doi.org/10.48550/arXiv.1301.3781)
2. [CS224N: Lecture 1](https://youtu.be/rmVRLeJRkl4?si=A7WMvluJe852etaM)
